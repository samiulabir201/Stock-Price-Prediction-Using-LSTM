{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34349,"databundleVersionId":3935619,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Processing\n\nThe majority of the code follows the provided template code in the [Competition's Train Demo Notebook](https://www.kaggle.com/code/smeitoma/train-demo/notebook), but this notebook uses an **LSTM model** to make stock price predictions instead of the Demo's LGBM model.\n\nThe following functions are used to adjust the close prices in the raw stock price data.","metadata":{}},{"cell_type":"markdown","source":"We import the code necessary for the LSTM Model.","metadata":{}},{"cell_type":"markdown","source":"We load the data, adjust closing prices, and view the resulting data.","metadata":{}},{"cell_type":"code","source":"from decimal import ROUND_HALF_UP, Decimal\n\ndef adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n\n    price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load stock price data\ndf_price = pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\n# df_supp =  pd.read_csv(\"/kaggle/input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_price = adjust_price(df_price)\n# df_supp = adjust_price(df_supp)\ndf_price.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_price.head(2))\ndisplay(df_price.tail(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following function represents **Feature Engineering**, and we can use this to set features for the stock price data.","metadata":{}},{"cell_type":"code","source":"def get_features_for_predict(price, code):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n        code (int)  : A local code for a listed company\n    Returns:\n        feature DataFrame (pd.DataFrame)\n    \"\"\"\n    close_col = \"AdjustedClose\"\n    feats = price.loc[price[\"SecuritiesCode\"] == code, [\"SecuritiesCode\", close_col, \"ExpectedDividend\", \"High\", \"Low\"]].copy()\n\n    # calculate return using AdjustedClose\n    feats[\"return_1day\"] = feats[close_col].pct_change(1)\n    # calculate 2 week return using AdjustedClose\n    feats[\"return_2week\"] = feats[close_col].pct_change(10)\n    # calculate last 1 month return using AdjustedClose\n    feats[\"return_1month\"] = feats[close_col].pct_change(21)\n    # calculate last 3 months return using AdjustedClose\n    feats[\"return_3month\"] = feats[close_col].pct_change(63)\n\n    # calculate 2 week historical volatility using AdjustedClose\n    feats[\"volatility_2week\"] = (\n        np.log(feats[close_col]).diff().rolling(10).std()\n    )\n    # calculate last 1 month historical volatility using AdjustedClose\n    feats[\"volatility_1month\"] = (\n        np.log(feats[close_col]).diff().rolling(21).std()\n    )\n    # calculate last 3 months historical volatility using AdjustedClose\n    feats[\"volatility_3month\"] = (\n        np.log(feats[close_col]).diff().rolling(63).std()\n    )\n\n    # ExpectedDividend\n    feats[\"ExpectedDividend\"] = feats[\"ExpectedDividend\"].mask(feats[\"ExpectedDividend\"] > 0, 1)\n\n    # Amplitude\n    feats[\"Amplitude\"] = feats[\"High\"] - feats[\"Low\"]\n\n    # RSI\n    C_Diff = feats['AdjustedClose'] - feats['AdjustedClose'].shift(1)\n    U = C_Diff.apply(lambda series: series if series > 0 else 0)\n    D = C_Diff.apply(lambda series: -series if series < 0 else 0)\n    EMA_U = U.ewm(span = 10, adjust = False).mean()\n    EMA_D = D.ewm(span = 10, adjust = False).mean()\n    RSI = EMA_U/(EMA_U+EMA_D) * 100\n    RSI.rename('RSI',inplace = True)\n    feats = feats.merge(RSI,left_index = True,right_index = True,how = 'left')\n\n    # 52 Week High\n    High52 = feats['AdjustedClose']/feats['High'].rolling(250).max()\n    High52.rename('High52',inplace = True)\n    feats = feats.merge(High52,left_index = True,right_index = True, how = 'left')\n\n    # BIAS\n    BIAS = feats['AdjustedClose'].rolling(10).mean()\n    BIAS = (feats['AdjustedClose'] - BIAS)/BIAS\n    BIAS.rename('BIAS' + str(10),inplace = True)\n    feats = feats.merge(BIAS,left_index = True,right_index = True, how = 'left')\n\n    # filling data for nan and inf\n    feats = feats.fillna(0)\n    feats = feats.replace([np.inf, -np.inf], 0)\n    # drop AdjustedClose column\n    feats = feats.drop([close_col], axis=1)\n\n    return feats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fetch prediction target SecuritiesCodes\n# There are 2000 codes\ncodes = sorted(df_price[\"SecuritiesCode\"].unique())\nlen(codes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# generate the features for prediction\nbuff = []\nfor code in tqdm(codes):\n    feat = get_features_for_predict(df_price, code)\n    buff.append(feat)\nfeature = pd.concat(buff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(feature.head(2))\ndisplay(feature.tail(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define a function to obtain labels for the data.","metadata":{}},{"cell_type":"code","source":"def get_label(price, code):\n    \"\"\" Labelizer\n    Args:\n        price (pd.DataFrame): dataframe of stock_price.csv\n        code (int): Local Code in the universe\n    Returns:\n        df (pd.DataFrame): label data\n    \"\"\"\n    df = price.loc[price[\"SecuritiesCode\"] == code].copy()\n    df.loc[:, \"label\"] = df[\"Target\"]\n\n    return df.loc[:, [\"SecuritiesCode\", \"label\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split the data into **Train** and **Test** sets. This can also be updated to obtain **Validation** sets later on.","metadata":{}},{"cell_type":"code","source":"# split data into TRAIN and TEST\nTRAIN_END = \"2019-12-31\"\n# We put a week gap between TRAIN_END and TEST_START\n# to avoid leakage of test data information from label\nTEST_START = \"2020-01-06\"\n\ndef get_features_and_label(price, codes, features):\n    \"\"\"\n    Args:\n        price (pd.DataFrame): loaded price data\n        codes  (array) : target codes\n        feature (pd.DataFrame): features\n    Returns:\n        train_X (pd.DataFrame): training data\n        train_y (pd.DataFrame): label for train_X\n        test_X (pd.DataFrame): test data\n        test_y (pd.DataFrame): label for test_X\n    \"\"\"\n    # to store splited data\n    trains_X, tests_X = [], []\n    trains_y, tests_y = [], []\n\n    # generate feature one by one\n    for code in tqdm(codes):\n\n        feats = features[features[\"SecuritiesCode\"] == code].dropna()\n        labels = get_label(price, code).dropna()\n\n        if feats.shape[0] > 0 and labels.shape[0] > 0:\n            # align label and feature indexes\n            labels = labels.loc[labels.index.isin(feats.index)]\n            feats = feats.loc[feats.index.isin(labels.index)]\n\n            assert (labels.loc[:, \"SecuritiesCode\"] == feats.loc[:, \"SecuritiesCode\"]).all()\n            labels = labels.loc[:, \"label\"]\n\n            # split data into TRAIN and TEST\n            _train_X = feats[: TRAIN_END]\n            _test_X = feats[TEST_START:]\n\n            _train_y = labels[: TRAIN_END]\n            _test_y = labels[TEST_START:]\n            \n            assert len(_train_X) == len(_train_y)\n            assert len(_test_X) == len(_test_y)\n\n            # store features\n            trains_X.append(_train_X)\n            tests_X.append(_test_X)\n            # store labels\n            trains_y.append(_train_y)\n            tests_y.append(_test_y)\n            \n    # combine features for each codes\n    train_X = pd.concat(trains_X)\n    test_X = pd.concat(tests_X)\n    # combine label for each codes\n    train_y = pd.concat(trains_y)\n    test_y = pd.concat(tests_y)\n\n    return train_X, train_y, test_X, test_y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate feature/label\ntrain_X, train_y, test_X, test_y = get_features_and_label(\n    df_price, codes, feature\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.loc[:, 'Target'] = train_y\ntest_X.loc[:, 'Target'] = test_y\ntrain_X.reset_index(inplace = True)\ntest_X.reset_index(inplace = True)\ndf_price.reset_index(inplace = True)\ntrain_X = df_price[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(train_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\ntest_X = df_price[[\"Date\", \"SecuritiesCode\", \"Open\", \"Close\", \"Volume\"]].merge(test_X, left_on = ['Date', 'SecuritiesCode'],right_on = ['Date', 'SecuritiesCode'], how = 'right')\ntrain_X.dropna(inplace = True)\ntest_X.dropna(inplace = True)\n\ntrain_y = train_X['Target']\ntest_y = test_X['Target']\ntrain_X.drop(\"Target\", axis = 1, inplace = True);\ntest_X.drop(\"Target\", axis = 1, inplace = True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.set_index('Date', inplace = True)\ntest_X.set_index('Date', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_test_X = test_X\nold_train_X = train_X\nfeat_cols = list(range(1, 18))\ntrain_X = train_X.iloc[:, feat_cols]\ntest_X = test_X.iloc[:, feat_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_X.values\nX_test = test_X.values\ny_train = train_y.values\ny_test = test_y.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train_X has shape\", X_train.shape)\nprint(\"train_y has shape\", y_train.shape)\nprint(\"test_X has shape\", X_test.shape)\nprint(\"test_y has shape\", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Training\n\nWe create the model; this is where we can design the neural network's structure and parameters.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor i in range(X_train.shape[1]):\n    plt.scatter(X_train[:, i], train_y)\n    plt.xlabel(train_X.columns[i])\n    plt.ylabel('y_train')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LassoCV, Lasso, LinearRegression\n\nreg_params = 10**(np.linspace(-8, -0.5, 100))\nnum_params = len(reg_params)\n\nmodel_lasso = LassoCV(n_alphas = num_params, alphas = reg_params,\n                      cv = 5, random_state = 0).fit(X_train, y_train)\nmodel_LR = LinearRegression().fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nbest_lasso_alpha = model_lasso.alpha_\nlasso_train_r2 = model_lasso.score(X_train, y_train)\nlasso_train_pred = model_lasso.predict(X_train)\nlasso_test_r2= model_lasso.score(X_test, y_test)\nlasso_test_pred = model_lasso.predict(X_test)\n\nLR_train_r2 = model_LR.score(X_train, y_train)\nLR_train_pred = model_LR.predict(X_train)\nLR_test_r2 = model_LR.score(X_test, y_test)\nLR_test_pred = model_LR.predict(X_test)\n\ntrain_lasso_rmse = np.sqrt(mean_squared_error(y_train, lasso_train_pred))\ntest_lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_test_pred))\ntrain_LR_rmse = np.sqrt(mean_squared_error(y_train, LR_train_pred))\ntest_LR_rmse = np.sqrt(mean_squared_error(y_test, LR_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Lasso training R^2 for, {best_lasso_alpha:0.2e}, is: {lasso_train_r2:0.2e}\")\nprint(f\"Lasso training RMSE for, {best_lasso_alpha:0.2e}, is: {train_lasso_rmse:0.2e}\")\nprint(f\"Lasso test R^2 for, {best_lasso_alpha:0.2e}, is: {lasso_test_r2:0.2e}\")\nprint(f\"Lasso test RMSE for, {best_lasso_alpha:0.2e}, is: {test_lasso_rmse:0.2e}\")\nprint(f\"LR training R^2 is: {LR_train_r2:0.2e}\")\nprint(f\"LR training RMSE is: {train_LR_rmse:0.2e}\")\nprint(f\"LR test R^2 is: {LR_test_r2:0.2e}\")\nprint(f\"LR test RMSE is: {test_LR_rmse:0.2e}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nxgb_model = XGBRegressor()\n\ncv_params = {\n    'n_estimators': [100, 200],\n    'max_depth': np.linspace(2, 10, 4, dtype = int),\n    'learning_rate': 10**(np.linspace(-4, 0, 4)),\n}\n\nxgb_gs = GridSearchCV(estimator = xgb_model, param_grid = cv_params, error_score = 'raise')\nxgb_gs.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = xgb_gs.best_params_\ntrain_xgb_pred = xgb_gs.predict(X_train)\ntest_xgb_pred = xgb_gs.predict(X_test)\ntrain_xgb_r2 = xgb_gs.score(X_train, y_train)\ntest_xgb_r2 = xgb_gs.score(X_test, y_test)\ntrain_xgb_rmse = np.sqrt(mean_squared_error(y_train, train_xgb_pred))\ntest_xgb_rmse = np.sqrt(mean_squared_error(y_test, test_xgb_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The best parameters for XGBRegressor is: {best_params}\")\nprint(f\"XGBoost training R^2 is: {train_xgb_r2:0.2e}\")\nprint(f\"XGBoost training RMSE is: {train_xgb_rmse:0.2e}\")\nprint(f\"XGBoost test R^2 is: {test_xgb_r2:0.2e}\")\nprint(f\"XGBoost test RMSE is: {test_xgb_rmse:0.2e}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_rank(df):\n    \"\"\"\n    Args:\n        df (pd.DataFrame): including predict column\n    Returns:\n        df (pd.DataFrame): df with Rank\n    \"\"\"\n    # sort records to set Rank\n    df = df.sort_values(\"Predict\", ascending=False)\n    # set Rank starting from 0\n    df.loc[:, \"Rank\"] = np.arange(len(df[\"Predict\"]))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = old_test_X[[\"SecuritiesCode\"]].copy()\nresult.loc[:, \"Predict\"] = lasso_test_pred\nresult.loc[:, 'Target'] = test_y.values\n\nresult = result.sort_values([\"Date\", \"Predict\"], ascending=[True, False])\nresult = result.groupby(\"Date\").apply(set_rank)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(calc_spread_return_sharpe(result, portfolio_size=200))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = old_test_X[[\"SecuritiesCode\"]].copy()\nresult.loc[:, \"Predict\"] = xgb_gs.predict(X_test)\nresult.loc[:, 'Target'] = test_y.values\n\nresult = result.sort_values([\"Date\", \"Predict\"], ascending=[True, False])\nresult = result.groupby(\"Date\").apply(set_rank)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(calc_spread_return_sharpe(result, portfolio_size=200))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\ntest_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"After Expanding Dimensions for LSTM:\")\nprint(\"train_X has shape\", train_X.shape)\nprint(\"test_X has shape\", test_X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\nmodel.add(LSTM(units = 64,input_shape = (7, 1)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1))\n\nmodel.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\n\n# model.add(LSTM(50, return_sequences=True, input_shape = (7, 1)))\n# model.add(LSTM(50, return_sequences=False))\n# model.add(Dense(25))\n# model.add(Dense(1))\n# model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_X[:, :, :],train_y,batch_size = 4096,epochs = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict\nLSTM_test_pred = model.predict(test_X[:, :, :])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = old_test_X[[\"SecuritiesCode\"]].copy()\nresult.loc[:, \"Predict\"] = LSTM_test_pred\nresult.loc[:, 'Target'] = test_y.values\n\nresult = result.sort_values([\"Date\", \"Predict\"], ascending=[True, False])\nresult = result.groupby(\"Date\").apply(set_rank)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(calc_spread_return_sharpe(result, portfolio_size=200))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_train_X.iloc[:, feat_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_test_X.iloc[:, feat_cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Open Close Volume ExpectedDividend High Low:\n\nLasso: 0.06492441599033187\n\nXGBoost: 0.08032163291177492\n\nLSTM: 0.056728157347210165","metadata":{}},{"cell_type":"markdown","source":"# Open Close Volume ExpectedDividend High Low:\n\nLasso: \n\nXGBoost: \n\nLSTM: ","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}}]}